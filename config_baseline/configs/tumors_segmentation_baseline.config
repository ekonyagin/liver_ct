from dpipe_configs.torch import *

from dpipe.dataset.segmentation import SegmentationFromCSV
from dpipe.dataset.wrappers import apply, cache_methods_to_disk, cache_methods

from pathlib import Path
import numpy as np

from dpipe.medim.shape_utils import prepend_dims
from dpipe.medim.metrics import dice_score, convert_to_aggregated, hausdorff_distance
from dpipe.split.cv import train_val_test_split

from functools import partial
from dpipe.batch_iter import load_by_random_id, Infinite, combine_pad
from dpipe.batch_iter.utils import zip_apply, multiply, unpack_args, apply_at

import torch
from torch.nn import functional
from torch.optim import Adam
from torch import nn

import dpipe.layers as lrs
from dpipe.torch import inference_step
from dpipe.train import NamedTBLogger, TimeProfiler, Constant, Schedule, LambdaEpoch

from dpipe.im.preprocessing import get_greatest_component

# ----- datasets
from dpipe.im.patch import get_random_patch

from liver_ct.dataset import resize_image, resize_binary_mask, get_patch_distribution, resize_multilabel_mask, \
    split_liver_tumor_mask
from liver_ct.metrics import multichannel_score

data_path = Path('')
liver_pred_path = Path('')
cache_path = Path('')


raw_dataset = apply(CroppedDataset(data_path=data_path,
                                  liver_pred_path=liver_pred_path,
                                  modalities=['CT'],
                                  target='target',
                                  metadata_rpath=''meta_with_preds.csv''),
                    # remove empty dim
                    load_image=lambda x: x.astype(np.float),
                    # get only 1 label for tumors
                    
                    load_segm=lambda x: x.astype(np.float))

clipped_dataset = apply(raw_dataset, load_image=partial(np.clip, a_min=-1000, a_max=2000))
zoom_factor = np.array([0.25, 0.25, 0.5])

#changed n_classto 1
mask_splitter = partial(split_liver_tumor_mask, n_class=1)

resized_dataset = apply(clipped_dataset,
    load_image=partial(resize_image, factor=zoom_factor),
    load_segm=partial(resize_multilabel_mask, factor=zoom_factor, multilabel_mask_splitter=mask_splitter))


cached_dataset = cache_methods_to_disk(apply(resized_dataset,
                                             load_image=np.float16, load_segm=lambda x: x.astype(np.uint8)),
                base_path=cache_path / 'LITS_scaled_0.25_0.25_0.5',
                load_image='CT', load_segm='multilabel_segm')


dataset = apply(cache_methods(cached_dataset, methods=['load_image', 'load_segm']),
            load_image=np.float32, load_segm=np.float32)

load_x = dataset.load_image
load_y = dataset.load_segm

# ----- batch iterator

batch_iter = Infinite(
    load_by_random_id(load_x, load_y, ids=train_ids, random_state=42),
    #multiply(np.float32),
    #unpack_args(lambda x, y: get_random_patch(x, y, patch_size=min(patch_size, x.shape[-1]), axes=(-1,),
    #            distribution=get_patch_distribution(y, delta=50, plus=1))),
    unpack_args(lambda x, y: get_random_patch(x, y, patch_size=min(patch_size, x.shape[-1]), axes=(-1,))),
    #multiply(prepend_dims),
    apply_at(0, prepend_dims),
    apply_at(1, lambda x: x.astype(np.int64)),
    batch_size=batch_size,
    batches_per_epoch=n_samples_per_epoch // batch_size,
    combiner=partial(combine_pad, padding_values=np.min))

# ----- model
device = 'cuda'
params = dict(kernel_size=3, padding=1)

upsample = partial(nn.Upsample, scale_factor=2, mode='trilinear')
downsample = partial(nn.MaxPool3d, kernel_size=2)

structure = [
    [[8, 8, 16], [32, 16, 3]],
    [[16, 32, 32], [64, 32, 16]],
    [[32, 64, 64], [128, 64, 32]],
    #[[32, 64, 128], [256, 128, 32]],
    [64, 128, 128, 64],
]

architecture = nn.Sequential(
    nn.Conv3d(1, 8, **params),
    lrs.FPN(
        lrs.ResBlock3d, downsample=nn.MaxPool3d(2), upsample=nn.Identity(),
        merge=lambda left, down: torch.cat(lrs.interpolate_to_left(left, down, 'trilinear'), 1),
        structure=structure, **params
    ),
).to(device)

final_activation = nn.Softmax(dim=1)
criterion = nn.CrossEntropyLoss()
predict = lambda x: inference_step(x[None, None, ...], architecture=architecture, activation=final_activation)[0]


train_kwargs = dict(lr=lr, architecture=architecture, optimizer=optimizer,
                    criterion=criterion, time=TimeProfiler(logger.logger))

# ----- loggers
#CheckpointManager

checkpoint_manager = Checkpoints(checkpoints_path, {
    'model.pth': architecture, 'optimizer.pth': optimizer
}, frequency=10)

# ----- metrics
multichannel_dice = partial(multichannel_score, score=dice_score, multilabel_mask_splitter=mask_splitter)
multichannel_hausdorff = partial(multichannel_score, score=hausdorff_distance, multilabel_mask_splitter=mask_splitter)

metrics = {'dice': multichannel_dice,
           'hausdorff': multichannel_hausdorff}


val_metrics = convert_to_aggregated(metrics, partial(np.nanmean, axis=0))
# ----- split

# ids filtering
df = dataset.df
ids = sorted(df[df.SliceSpacing<=1].index)


split = train_val_test_split(
    ids=ids,
    val_size=6, n_splits=6
)[:1]#[3:4]

# ----- params
initial = 1e-3
lr = Schedule(initial, {40:0.5, 70:0.5})

optimizer = Adam(architecture.parameters())

batch_size = 4
n_samples_per_epoch = len(train_ids)*5
n_epochs = 160

patch_size = 100

metrics_path = 'test_metrics'
saved_model_path = 'model.pth'
test_predictions_path = 'test_predictions'


run_experiment = (
    lock_dir(),
    populate(saved_model_path, lambda: [train_model, save_model_state(architecture, saved_model_path)]),
    load_model_state(architecture, saved_model_path),

    populate(test_predictions_path, commands.predict, ids=test_ids, output_path=test_predictions_path, load_x=load_x,
             predict_fn=predict),

    populate(metrics_path, commands.evaluate_individual_metrics, load_y, metrics, test_predictions_path,
             metrics_path),
    populate(metrics_path, commands.evaluate_aggregated_metrics, load_y, val_metrics, test_predictions_path,
             metrics_path + '/aggregated_test_metrics', exist_ok=True),
)
